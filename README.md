A robust Amazon web scraper built with Scrapy and Selenium WebDriver, designed for extracting product listings dynamically rendered on Amazonâ€™s search pages.
 
 Features
ğŸ” Scrapes Amazon search result listings.

ğŸ§  Uses Selenium WebDriver to handle JavaScript-rendered content.

ğŸ›¡ï¸ Proxy support (e.g., Oxylabs or any rotating proxy provider).

ğŸª Cookie handling via parse_cookie_string() for authenticated sessions.

ğŸ§© Modular utility functions for reuse across spiders.

ğŸ§ª Includes timeout and wait handling for Amazon page load issues.

ğŸ—‚ï¸ Category-Based Scraping
  You can specify Amazon categories (e.g., Electronics, Books, Toys) to scrape product listings for targeted segments.
  
    Accepts category either from:
    CSV input (e.g., categories.csv)
    Hardcoded or passed via command-line arguments
    Dynamically builds Amazon search URLs like: https://www.amazon.com/s?k=Headphones&i=electronics

 âš™ï¸ Setup Instructions
  git clone https://github.com/your-username/amazon-scraper.git
  cd amazon-scraper

  Install Dependencies:
  pip install -r requirements.txt

  Install WebDriver (e.g., ChromeDriver) Ensure your system has ChromeDriver installed and matches your browser version.

  ğŸ§ª Run the Spider:
  scrapy crawl main_categories
    scrapy crawl main_categories -o output.csv
  Extract data category wise
    scrapy crawl products -a main_category=Electronics -a sub_category=Headphones -o output.csv

 ğŸ” Proxy & Cookie Setup:
   In oditworks_scraper_utils.py, you can:

    Define your Oxylabs or other proxy credentials.
    
    Use parse_cookie_string(raw_cookie) to load cookies from raw headers and inject them into the Selenium driver.

ğŸ“ Example Utility Functions
    parse_cookie_string(cookie_string)
    
    get_random_user_agent()
    
    wait_for_amazon_products(driver, timeout, logger

ğŸ›  Requirements:
  Python 3.8+
  Scrapy
  Selenium
  Chrome + ChromeDriver
  (Optional) Proxy provider account

âœ… Disclaimer
This project is for educational and research purposes only. Scraping Amazon may violate their terms of service. Use at your own risk.

